#!/usr/bin/env python
# -*- coding: utf-8 -*-
# vim: set autoindent smartindent softtabstop=4 tabstop=4 shiftwidth=4 noexpandtab:
###############################################################################
###
### This little script allows to fetch the zone files from ipdeny.com into
### a local directory in order to use it in iptables rules and similar
### scenarios.
###
### The script heeds robots.txt, identifies itself and will be conservative
### with the remote resources by checking the timestamp given on the index page
### at ipdeny.com. If the timestamp is newer or no local version exists, the
### file will be downloaded. Downloaded files are timestamped to match the
### timestamps on the remote server. This allows for Makefiles and similar
### gadgets to work properly.
### The script limits its scope to files named XX.zone where XX are any two
### lowercase Latin letters.
###
### Please use the script mindfully, if you come across it and accept the terms
### of use at ipdeny.com. See:
###       <http://www.ipdeny.com/ipblocks/data/countries/Copyrights.txt>
###
###############################################################################
__author__ = "Oliver Schneider"
__copyright__ = "2014 Oliver Schneider (assarbad.net), under Public Domain/CC0, or MIT/BSD license where PD is not applicable"
__version__ = "1.0"
import hashlib, os, sys, time

# Checking for compatibility with Python version
if (sys.version_info[0] != 2) or (sys.version_info < (2,7)):
    sys.exit("This script requires Python version 2.7 or better from the 2.x branch of Python.")

# Additional modules
import argparse, mechanize, re
from HTMLParser import HTMLParser
from datetime import datetime
from os import path
from platform import platform, machine, python_version, python_implementation, system as sysnm, release as sysrel

debug = 0
base_url = "http://www.ipdeny.com/ipblocks/data/countries/"
override_headers = [
	("User-agent", "ipdeny-fetch-zones/%s (%s/%s %s; %s %s)" % (__version__, sysnm(), machine(), sysrel(), python_implementation(), python_version())),
	("Accept", "text/html, application/xhtml+xml, */*"),
	("Accept-Charset", ""),
	("Accept-Language", "en-US"),
	("Connection", "Keep-Alive"),
	]
mirror_basepath = path.join(path.dirname(path.realpath(__file__)), 'zones')

def touch(fname, times=None):
	with file(fname, 'a'):
		os.utime(fname, times)

def touchdt(fname, dt):
	numsecs = (dt - datetime(1970, 1, 1)).total_seconds()
	return touch(fname, (numsecs, numsecs))

def parse_index(html_page, baseurl, download_file):
	global debug
	# Replace HTML entities by their text form
	entities = re.findall("&.+?;", html_page)
	html = HTMLParser()
	for entity in entities:
		html_page = html_page.replace(entity, html.unescape(entity))
	urls = {}
	# Find all the links and the respective timestamps
	for m in re.finditer("<a\s+href=\"([a-z]{2}\.zone)\">[^<]+<\/a><\/td><td\s+align=\"right\">([^<]+)", html_page):
		if m:
			filename, datestr = m.group(1), m.group(2).strip()
			dt = datetime.strptime(datestr, "%d-%b-%Y %H:%M")
			if debug > 2:
				print "Parsed as: %s: %s" % (filename, dt)
			# Store the result, hashed by URL
			urls[("%s%s" % (baseurl, filename))] = {
					'fname': filename.lower(),
					'dt'   : dt,
				}
	failures = 0
	# Go through the list keyed by URL
	for url,info in urls.iteritems():
		global mirror_basepath
		fname = path.join(mirror_basepath, path.basename(info['fname']))
		try:
			failures += download_file(url, fname, info['dt'])
		except KeyboardInterrupt:
			os.unlink(fname)
			failures += 1
			print "Caught signal. Removed partial file %s." % (fname)
			break
	return failures

def get_index(**kwargs):
	global debug
	global base_url
	global mirror_basepath
	global override_headers
	if 'debug' in kwargs:
		debug = kwargs['debug']
		if debug:
			print "Debug level %d" % (debug)
	if 'directory' in kwargs:
		if mirror_basepath != kwargs['directory']:
			if debug:
				print "Output directory is: %s" % (mirror_basepath)
			mirror_basepath = kwargs['directory']
	br = mechanize.Browser()
	br.set_handle_equiv(True)
	br.set_handle_redirect(True)
	br.set_handle_referer(True)
	br.addheaders = override_headers
	if debug > 1:
		br.set_debug_http(True)
		br.set_debug_redirects(True)
		br.set_debug_responses(True)
	response = br.open(base_url)
	# Create download directory if it doesn't exist
	if not path.exists(mirror_basepath) and not path.isdir(mirror_basepath):
		os.mkdir(mirror_basepath)
	# Callback for downloading an individual file
	def download_file(url, fname, remotedt):
		# Check whether we need to download the file
		if path.exists(fname):
			locdt = datetime.fromtimestamp(path.getmtime(fname))
			if locdt >= remotedt:
				if debug > 1:
					print "%s remote not newer than local (R:%s <= L:%s)" % (path.basename(fname), remotedt, locdt)
				return 0
		# Download the file
		try:
			if br.retrieve(url, fname):
				print "Downloaded: %s" % fname
				touchdt(fname, remotedt)
				return 0
		except (mechanize.HTTPError,mechanize.URLError) as e:
			if isinstance(e,mechanize.HTTPError):
				print "HTTP error code %d [%s]" % (e.code, url)
				if path.exists(fname):
					os.unlink(fname)
			else:
				print "ERROR: %s " % str(e.reason.args)
		return 1
	# Now parse the index HTML file
	return parse_index(response.read(), base_url, download_file)

def main(**kwargs):
	get_index(**kwargs)

def parse_args():
	global mirror_basepath
	from argparse import ArgumentParser
	parser = ArgumentParser(description='Mirror script for %s' % (base_url))
	parser.add_argument('--version', '-V', action='version', version=__version__,
						help='show the program version and exit')
	parser.add_argument('--debug', '-d', action='count', default=0,
						help='turn on debugging (more extensive logging) and increase detail')
	parser.add_argument('--directory', '-D', action='store', default=mirror_basepath,
						help='give a directory to download the zone files to')
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	sys.exit(main(**vars(args)))
