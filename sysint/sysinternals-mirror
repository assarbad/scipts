#!/usr/bin/env python
# -*- coding: utf-8 -*-
# vim: set autoindent smartindent softtabstop=4 tabstop=4 shiftwidth=4 noexpandtab:
__author__ = "Oliver Schneider"
__copyright__ = "Copyright (C) 2013 Oliver Schneider (assarbad.net), under Public Domain/CC0"
__version__ = "1.0"
import hashlib, os, sys, time

# Checking for compatibility with Python version
if (sys.version_info[0] != 2) or (sys.version_info < (2,7)):
    sys.exit("This script requires Python version 2.7 or better from the 2.x branch of Python.")

# We have two modules stored relative to this script path
sys.path.append(os.path.dirname(os.path.realpath(__file__)))
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'verify-sigs'))
# Additional modules
import argparse, mechanize, re, pyasn1, auth_data, fingerprint, pecoff_blob
from HTMLParser import HTMLParser
from datetime import datetime
from os import path

debug = 0
sysint_url = "http://live.sysinternals.com"
override_headers = [
	("User-agent", "Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko"),
	("Accept", "text/html, application/xhtml+xml, */*"),
	("Accept-Charset", ""),
	("Accept-Language", "en-US"),
	("Connection", "Keep-Alive"),
	]
mirror_basepath = path.join(path.dirname(path.realpath(__file__)), 'live.sysint')

def verify_signature(fname):
	with open(fname, 'rb') as f:
		fp = fingerprint.Fingerprinter(f)
		is_pecoff = fp.EvalPecoff()
		fp.EvalGeneric()
		results = fp.HashIt()
		if is_pecoff:
			fingerprint.FindPehash(results)
		else:
			return True # not a PE file
	return False

def parse_index(html_page, baseurl, download_file):
	# Get everything inside the <pre> element
	match = re.search("<pre>(.+)</pre>", html_page, re.DOTALL | re.IGNORECASE)
	if not match:
		return 1 # error
	# Replace links by text, split at converted line breaks
	result = re.sub(r'(?si)<a\s+href="([^"]+)">[^<]+</a><br>', "\\1\n", match.group(1))
	# Now replace HTML entities by their text form
	entities = re.findall("&.+?;", result)
	html = HTMLParser()
	for entity in entities:
		result = result.replace(entity, html.unescape(entity))
	urls = {}
	# Now go over them one by one
	for line in result.split("\n"):
		# Tokenize the lines
		match = re.search("(.{38})\s+(\d+|<dir>)\s+([^\s\n]+)", line, re.IGNORECASE)
		if match:
			# Get a nice representation
			(datestr, sizestr, filename) = (match.group(1).strip(), match.group(2).strip(), match.group(3).strip())
			# Be picky about the items we want
			if re.search(r"\.(exe|scr|dll|sys|chm|hlp)$", filename, re.IGNORECASE) and not re.search(r"pdh\.dll$", filename, re.IGNORECASE):
				# Convert date/time string to a format we can handle more easily
				dt = datetime.strptime(datestr, "%A, %B %d, %Y %I:%M %p")
				# Store the result, hashed by URL
				urls[("%s%s" % (baseurl, filename))] = {
						'size' : int(sizestr),
						'fname': filename.lower(),
						'dt'   : dt,
						'line' : line,
					}
	failures = 0
	# Go through the list keyed by URL
	for url,info in urls.iteritems():
		global mirror_basepath
		fname = path.join(mirror_basepath, path.basename(info['fname']))
		ret = download_file(url, fname, info['dt'], info['size'])
		if 0 == ret:
			rsize = path.getsize(fname)
			if not rsize == info['size']:
				print "ERROR: file size %d does not match expected value %d (%s)!" % (rsize, info['size'], fname)
				os.unlink(fname)
				failures += 1
				continue
			if not verify_signature(fname):
				print "ERROR: verification of AuthentiCode signature failed (%s)!" % (fname)
				os.unlink(fname)
				failures += 1
				continue
		else:
			failures += ret
	return failures

def get_index(**kwargs):
	if 'debug' in kwargs:
		global debug
		debug = kwargs['debug']
	br = mechanize.Browser()
	br.set_handle_robots(False)
	br.set_handle_equiv(True)
	br.set_handle_redirect(True)
	br.set_handle_referer(True)
	global override_headers
	br.addheaders = override_headers
	if debug > 0:
		br.set_debug_http(True)
		br.set_debug_redirects(True)
		br.set_debug_responses(True)
	global sysint_url
	response = br.open(sysint_url)
	# Create download directory if it doesn't exist
	if not path.exists(mirror_basepath) and not path.isdir(mirror_basepath):
		os.mkdir(mirror_basepath)
	# Callback for downloading an individual file
	def download_file(url, fname, remotedt, fsize):
		global debug
		# Check whether we need to download the file
		if path.exists(fname):
			locdt = datetime.fromtimestamp(path.getmtime(fname))
			if locdt >= remotedt:
				if debug:
					print "%s not newer than remote" % fname
				return 0
		# Download the file
		try:
			if br.retrieve(url, fname):
				print "Downloaded: %s" % fname
		except (mechanize.HTTPError,mechanize.URLError) as e:
			if isinstance(e,mechanize.HTTPError):
				print "HTTP error code %d [%s]" % (e.code, url)
				if path.exists(fname):
					os.unlink(fname)
			else:
				print "ERROR: %s " % str(e.reason.args)
			return 1
		return 0
	# Now parse the index HTML file
	return parse_index(response.read(), sysint_url, download_file)

def main(**kwargs):
	get_index(**kwargs)

def parse_args():
	from argparse import ArgumentParser
	parser = ArgumentParser(description='Mirror script for live.sysinternals.com')
	parser.add_argument('--version', '-V', action='version', version=__version__,
						help='show the program version and exit')
	parser.add_argument('--debug', '-d', action='count', default=0,
						help='turn on debugging (more extensive logging) and increase detail')
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	sys.exit(main(**vars(args)))
